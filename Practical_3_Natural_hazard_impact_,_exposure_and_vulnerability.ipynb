{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/37stu37/GColab/blob/main/Practical_3_Natural_hazard_impact_%2C_exposure_and_vulnerability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical 3: Natural hazard impact , exposure and vulnerability\n",
        "\n",
        "## Introduction\n",
        "In this practical, you will model the impacts that are expected to result if the 1934 Mw 8.3 Nepal-Bihar earthquake were to (re-)occur today. You will use shaking data and population data provided to analyse the number of fatalities expected and compare the frequency-magnitude distributions to impacts from the 2015 Gorkha earthquake in Nepal.\n",
        "\n",
        "##Aims\n",
        "1. To work with essential GIS data into Python (Google Colab)\n",
        "2. To extract statistics from GIS raster layers\n",
        "3. To calculate the probability of building damage state \n",
        "4. To estimate fatalities from a disaster \n",
        "5. To create comparative study and reflect on the uncertainty and limitations of your data\n",
        "\n",
        "##Format of the assignment\n",
        "You need to write a summary of your findings in a short scientific abstract of no more than 200 words. The abstract should provide a thorough summary of the work undertaken and should be well-written and organised. \n",
        "\n",
        "The abstract / executive summary should provide:\n",
        "\n",
        "- An understanding of the fatality numbers from the 1934 and 2015 earthquake. How does it relate to building damages?\n",
        "- Based on the data you have, a comparative study of the two events. How do the data from the 2015 earthquake compare to the 1934 earthquake and what does this tell you about both events?\n",
        "- Explain the limitations of the current assumptions. What is the effect of those assumptions and how is this likely to be wrong? What impact could this have on your scenario results? \n",
        "\n",
        "##Submitting the assignment\n",
        "You will answer the questions and write your free form abstract in a separate word document / text file (graphs and maps are welcome to illustrate your point) that must be 200 words maximum. The document must be saved as a pdf and uploaded to Turnitin.\n",
        "\n",
        "The assignment is due by **7th February 2023 4pm**.\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "❗ <font color='red'>**Before going any further - Please save the notebook in your OWN Google Drive**</font> ❗\n",
        "\n",
        "\n",
        "<br/><br/>\n",
        "\n"
      ],
      "metadata": {
        "id": "QmBzWd80SuGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started\n",
        "\n",
        "First, you need to install the packages using pip.\n",
        "the *!* is used by Google Colab to understand that it needs to run a shell command"
      ],
      "metadata": {
        "id": "rUaCjfNay6fE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26e_c3pwXvGt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rioxarray\n",
        "!pip install geopandas\n",
        "!pip install rasterstats\n",
        "!pip install mapclassify\n",
        "!pip install plotly\n",
        "!pip install \"notebook>=5.3\" \"ipywidgets>=7.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, you need to load modules. The *import* tag install a library that can now be used in your script"
      ],
      "metadata": {
        "id": "sZozpk2Ky_Ua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWu9QTMTVSbq",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rioxarray as rio\n",
        "import plotly.express as px\n",
        "\n",
        "px.defaults.template = \"ggplot2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmxmkSWBVgDC"
      },
      "source": [
        "Before you begin, please read through the entire practical exercise. You may not understand everything, but you will have a better sense of what is expected of you and what you will be doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_15HXVYVsVO"
      },
      "source": [
        "## **Part I: Loading the data sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng0SctyHbyW2"
      },
      "source": [
        "### Access files in Goolge Colab \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How?**"
      ],
      "metadata": {
        "id": "RC_5RxKiJ-sG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C0zB4wcbvZi"
      },
      "source": [
        "Access local files through the file-explorer\n",
        "Uploading files from local file system through file-explorer\n",
        "You can either use the upload option at the top of the file-explorer pane to upload any file(s) from your local file system to Colab in the present working directory. \n",
        "\n",
        "To upload files directly to a subdirectory you need to:\n",
        "\n",
        "    1. Click on the three dots visible when you hover above the directory \n",
        "\n",
        "    2. Select the “upload” option.\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Now that you know how to load files we will download the data and load them in Google Colab:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "1. The Data folder is available in Blackboard. Copy each of the files to your disk\n",
        "\n",
        "2. The first step in scenario development is to model the ground shaking that will result from your chosen earthquake and compare it to population distribution. For this exercise I have modelled the shaking for the 1934 Mw 8.0 Nepal-Bihar earthquake for you (***PGA_1934EQ.tif***). I have also given you a shapefile of Nepali Village Development Committees (VDCs – similar to Local Authorities) that contain details on population and building type (***Nepal_VDCs.shp***). Note that Nepal’s local government system has changed recently, but we will use VDC-level data from the census in 2011. You have a also a csv file of the Fatalities per VDC after the 2015 Gorkha earthquake (***2015_EQData.csv***)\n",
        "\n",
        "3. Load those dataset in Google Colab as explain earlier (it will only remain temporarily on your drive)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access files in QGIS\n",
        "\n",
        "---\n",
        "\n",
        "Load the same datasets in QGIS following the subsequent steps:\n",
        "\n",
        "\n",
        "<pre>\n",
        "Click Layer\n",
        "    &darr; \n",
        "Add Layer\n",
        "    &darr; \n",
        "Add Vector (.shp)\n",
        "Add Raster (.tif)\n",
        "Add Delimited Text Layer (.csv)\n",
        "</pre>"
      ],
      "metadata": {
        "id": "YlyC4hMtlcDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The attribute table of the ***Nepal_VDCs.shp*** contains the name and a unique code for each VDC along with population data and the total number of different building types within that VDC. The column titles refer to the following:\n",
        "\n",
        "    WDN – Wooden (Bamboo or Timber)\n",
        "    RCNE – Non-Engineered Reinforced Concrete\n",
        "    BCF – Flexible Brick & Concrete\n",
        "    BCR – Rigid Brick & Concrete\n",
        "    SM – Stone & Mud\n",
        "\n",
        "\n",
        "The ***PGA_1934EQ.tif*** file is estimate of the peak ground accelerations from the 1934 earthquake, expressed as a fraction of the Earth’s gravitational acceleration, g. These are not observed values, but have been estimated using the OpenSHA model. You may wish to better visualise the shaking by playing with the Symbology of the PGA_1934EQ file in QGIS. We will also display all the data loaded in this notebook."
      ],
      "metadata": {
        "id": "iq7DMTFtlgJ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqaSgCGtgPjr"
      },
      "source": [
        "### Display the data in Google Colab\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpTUJQn6fOlT"
      },
      "source": [
        "Display the .tif raster file using rasterio/rioxarray library\n",
        "\n",
        "Rasterio and Rioxarray are Python options (not the only ones) for accessing the many different kind of raster data files used in the GIS field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_9V6SArYVBQ"
      },
      "outputs": [],
      "source": [
        "PGA_1934EQ = rio.open_rasterio('PGA_1934EQ.tif', masked=True)\n",
        "\n",
        "PGA_1934EQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms62ACWlgbTE"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Ground motion filed (PGA) of the 1934 earthquake \\n\")\n",
        "PGA_1934EQ.squeeze().plot.imshow();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MGqogjIdZ-z"
      },
      "source": [
        "There are different options to display csv and text files in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9852ZX1CeHu5"
      },
      "source": [
        " - using the Magics (%) capability of Google Colab (not the preferred approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpVzyt1aVved"
      },
      "outputs": [],
      "source": [
        "%cat 2015_EQData.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPXnE45EfVpb"
      },
      "source": [
        " - or the (very popular) Pandas library \n",
        "\n",
        "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iyqwCp1dnFv"
      },
      "outputs": [],
      "source": [
        "# using pandas library\n",
        "EQDdata = pd.read_csv(\"2015_EQData.csv\")\n",
        "display(EQDdata.head(5)) # head is used to select only the first n lines - here 5 lines with .head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A1fWBSSlf7u"
      },
      "source": [
        "Then the shapefile can be open using the Geopandas library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ErzpMBxlmq-"
      },
      "outputs": [],
      "source": [
        "nepal_VDCs = gpd.read_file(\"Nepal_VDCs.shp\")\n",
        "\n",
        "nepal_VDCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtGfu0NqosI0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,15))\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "\n",
        "# the column argument will be used to color code the display\n",
        "nepal_VDCs.plot(column='Pop_Total', legend=True, ax=ax, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0OvZ_3odZw"
      },
      "outputs": [],
      "source": [
        "# use the scheme argument to display a quantile classification\n",
        "nepal_VDCs.plot(column='Pop_Total', scheme='quantiles', legend=True, figsize=(10,15));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S2P9ItJe_A9"
      },
      "source": [
        "## Part II: Damage modelling\n",
        "\n",
        "In order to estimate the number of building collapses, first you need to assess the average shaking each VDC experiences in the scenario. Once you have estimated this, you will export the data to CSV to perform some modelling calculations.\n",
        "\n",
        "**IN QGIS:**\n",
        "\n",
        "QGIS can calculate the average values from a raster file within different zones by using a tool called Zonal Statistics. You will need to use Zonal Statistics to calculate the mean 1934 shaking within each VDC following these steps:\n",
        "\n",
        "\n",
        "<pre>\n",
        "Click Processing Toolbox\n",
        "          &darr; \n",
        "Zonal Statistics\n",
        "  &rarr; Input layer - VDC shapefile\n",
        "  &rarr; Raster layer - Earthquake raster\n",
        "          &darr; \n",
        "Remove \"_\" in the output column prefix\n",
        "          &darr; \n",
        "Calculate the Mean\n",
        "          &darr; \n",
        "Run and Save the output Shapefile\n",
        "          &darr; \n",
        "</pre>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Once the calculation has successfully completed, your new file should appear in the Layers. Right-click the file and select Open Attribute Table. \n",
        "\n",
        "To check the calculation has completed accurately, first make sure that the values in the Mean column appear sensible – remember, these are earthquake accelerations as a fraction of g.\n",
        "\n",
        "Then we need to export Stats to Google Colab:\n",
        "\n",
        "<pre>\n",
        "Right click on the output Shapefile file\n",
        "Export:\n",
        "  &rarr; Fromat = CSV\n",
        "  &rarr; CRS = EPSG 32645 - WGS84 / UTM Zone 45N\n",
        "  &rarr; Filename = \"stats.csv\"\n",
        "  </pre>\n",
        "\n",
        "You can now upload the ***stats.csv*** in Google Colab as you have done previously.\n",
        "\n",
        "And read it using *pd.read_csv*:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XZIm-V5xI0B"
      },
      "outputs": [],
      "source": [
        "vdc_stats = pd.read_csv(\"stats.csv\")\n",
        "display(vdc_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate the total building count by adding up the buildings of each type"
      ],
      "metadata": {
        "id": "znMCMM_e3oFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0WD4nbzxI0B"
      },
      "outputs": [],
      "source": [
        "vdc_stats[\"total building count\"] = vdc_stats[\"WDN\"] + vdc_stats[\"RCNE\"] + vdc_stats[\"BCF\"] + vdc_stats[\"BCR\"] + vdc_stats[\"SM\"]\n",
        "display(vdc_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dX5WxoW2fFv"
      },
      "source": [
        "Now you need to model building damage. To do this, you need to know how different types of buildings perform under shaking of different strengths.\n",
        "\n",
        "Empirical data from global earthquakes have been used to derive fragility curves for each of the building types in your data set. Below is a set of fragility curves that describe the rate of complete damage for each type. \n",
        "\n",
        "In this instance, *complete damage* refers to a state in which the structure has collapsed or is in imminent danger of collapse; all structural components (walls, floors, roofs etc.) are damaged and the building will need to be demolished and rebuilt entirely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4HazGX2j7Y"
      },
      "source": [
        "As a reminder, here are the typologies in the VDC attributes\n",
        "\n",
        "<pre>\n",
        "*   WDN - Wood frame\n",
        "*   RCNE - Reinforce Concrete\n",
        "*   BCF - Brick Flexible\n",
        "*   BCR - Brick Rigid\n",
        "*   SM - Stone & Mud\n",
        "</pre>\n",
        "\n",
        "The cumulative distribution of two-parameter lognormal distribution functions(mean and standard deviation) are used to represent the fragility curves.\n",
        "https://en.wikipedia.org/wiki/Log-normal_distribution\n",
        "\n",
        "\n",
        "We will use the lognorm module from scipy.stats library to sample the probability of complete damage state for a specific PGA(g) value of shaking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCxBMZHR10rM"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import lognorm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# get 100 artificial pga values between 0 and 3g\n",
        "pgas = np.linspace(0,3, 100)\n",
        "\n",
        "# two parameters - mean and standard deviation - of the lognormal fragility functions\n",
        "# for the different building typologies\n",
        "###########\n",
        "WDN_mean = 1.6446\n",
        "WDN_std = 1.1701\n",
        "\n",
        "RCNE_mean = 1.29\n",
        "RCNE_std = 0.83\n",
        "\n",
        "BCF_mean = 0.39\n",
        "BCF_std = 0.69\n",
        "\n",
        "BCR_mean = 1.26\n",
        "BCR_std = 1.96\n",
        "\n",
        "SM_mean = 0.23\n",
        "SM_std = 0.31\n",
        "############\n",
        "\n",
        "# Create the fragility curves for each typologies\n",
        "F_WDN = [lognorm(WDN_std,scale=WDN_mean).cdf(p) for p in pgas]\n",
        "F_RCNE = [lognorm(RCNE_std,scale=RCNE_mean).cdf(p) for p in pgas]\n",
        "F_BCF = [lognorm(BCF_std,scale=BCF_mean).cdf(p) for p in pgas]\n",
        "F_BCR = [lognorm(BCR_std,scale=BCR_mean).cdf(p) for p in pgas]\n",
        "F_SM = [lognorm(SM_std,scale=SM_mean).cdf(p) for p in pgas]\n",
        "\n",
        "# Plot the fragility functions\n",
        "alf=1\n",
        "fig, axs = plt.subplots(1, 1, figsize=(5,3))\n",
        "\n",
        "axs.plot(pgas, F_WDN, c='blue', label=\"WDN\", alpha=alf)\n",
        "axs.plot(pgas, F_RCNE, c='lightblue', label=\"RCNE\", alpha=alf)\n",
        "axs.plot(pgas, F_BCF, c='orange', label=\"BCF\", alpha=alf)\n",
        "axs.plot(pgas, F_BCR, c='lightgreen', label=\"BCR\", alpha=alf)\n",
        "axs.plot(pgas, F_SM, c='red', label=\"SM\", alpha=alf)\n",
        "\n",
        "\n",
        "axs.set(ylabel=\"Probability of complete damage\", xlabel=\"PGA (g)\")\n",
        "axs.set_ylim(0,1.05)\n",
        "axs.legend()\n",
        "axs.legend(loc='lower right', frameon=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then need, for each typologies, to calculate the probability of complete damage for the pga within the VDC"
      ],
      "metadata": {
        "id": "P_F9oDghtnUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7CseQREk_mg"
      },
      "outputs": [],
      "source": [
        "# calculate the fragility of each building types for each pgas\n",
        "pgas = vdc_stats[\"mean\"].values\n",
        "\n",
        "Complete_perc_WDN = [lognorm(WDN_std,scale=WDN_mean).cdf(p) for p in pgas]\n",
        "Complete_perc_RCNE = [lognorm(RCNE_std,scale=RCNE_mean).cdf(p) for p in pgas]\n",
        "Complete_perc_BCF = [lognorm(BCF_std,scale=BCF_mean).cdf(p) for p in pgas]\n",
        "Complete_perc_BCR = [lognorm(BCR_std,scale=BCR_mean).cdf(p) for p in pgas]\n",
        "Complete_perc_SM = [lognorm(SM_std,scale=SM_mean).cdf(p) for p in pgas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYXg332u4IIG"
      },
      "source": [
        "- Now you can calculate the number of buildings of each type, in each VDC, that are expected to suffer complete damage.\n",
        "\n",
        "- We can then calculate the total number of building in a *Complete* damage state (we round the number as decimal in building number count doesn't really make sense)\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EscD9ZSxxI0D"
      },
      "outputs": [],
      "source": [
        "# calculate the actual number of building predicted to be in a \"Complete damage state\"\n",
        "\n",
        "Complete_bldg_WDN = Complete_perc_WDN * vdc_stats[\"WDN\"].values\n",
        "Complete_bldg_RCNE = Complete_perc_RCNE * vdc_stats[\"RCNE\"].values\n",
        "Complete_bldg_BCF = Complete_perc_BCF * vdc_stats[\"BCF\"].values\n",
        "Complete_bldg_BCR = Complete_perc_BCR * vdc_stats[\"BCR\"].values\n",
        "Complete_bldg_SM = Complete_perc_SM * vdc_stats[\"SM\"].values\n",
        "\n",
        "vdc_stats[\"Complete_bldg_total\"] = Complete_bldg_WDN + Complete_bldg_RCNE + Complete_bldg_BCF + Complete_bldg_BCR + Complete_bldg_SM\n",
        "vdc_stats[\"Complete_bldg_total\"] = [round(x) for x in vdc_stats[\"Complete_bldg_total\"]]\n",
        "\n",
        "vdc_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9h6eYMExI0D"
      },
      "source": [
        "Typically, earthquake fatalities predominantly occur in buildings that collapse. \n",
        "\n",
        "You therefore need to estimate how many of your completely damaged buildings will fully collapse and thus how many buildings are expected to cause fatalities. \n",
        "\n",
        "Again, global empirical data for this has been collected and shows that different building types are more likely to collapse than others:\n",
        "\n",
        "| Building Type      | Probability that completely damaged building will collapse |\n",
        "| ----------- | ----------- |\n",
        "| WDN      | 3%       |\n",
        "| RCNE   | 13%        |\n",
        "| BCF      | 15%       |\n",
        "| BCR   | 15%        |\n",
        "| SM   | 15%        |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can calculate the number of actual collapse by multiplying the number of building in a Complete damage state with the probability of actual collapse\n",
        "\n",
        "- We can then add the results from all the collapse building to get a total per VDC"
      ],
      "metadata": {
        "id": "9WJJPeS44VIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiWT0L5xxI0E"
      },
      "outputs": [],
      "source": [
        "Collapse_bldg_WDN = Complete_bldg_WDN * 0.03\n",
        "Collapse_bldg_RCNE = Complete_bldg_RCNE * 0.13\n",
        "Collapse_bldg_BCF = Complete_bldg_BCF * 0.15\n",
        "Collapse_bldg_BCR = Complete_bldg_BCR * 0.15\n",
        "Collapse_bldg_SM = Complete_bldg_SM * 0.15\n",
        "\n",
        "vdc_stats[\"Collapse_bldg_total\"] = Collapse_bldg_WDN + Collapse_bldg_RCNE + Collapse_bldg_BCF + Collapse_bldg_BCR + Collapse_bldg_SM\n",
        "vdc_stats[\"Collapse_bldg_total\"] = [round(x) for x in vdc_stats[\"Collapse_bldg_total\"]]\n",
        "\n",
        "vdc_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxhH-kJh4Uwa"
      },
      "source": [
        "We can then plot the total numbers of building suffering complete damage as well as those that collapse. Compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN0dW3KVxI0E"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "barWidth = 0.2\n",
        "fig = plt.subplots(figsize =(3, 5))\n",
        "\n",
        "# Set position of bar on X axis\n",
        "br1 = 1\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(br1, np.sum(vdc_stats[\"total building count\"].values), color ='white', edgecolor ='grey', label ='Total building count')\n",
        "plt.bar(br1, np.sum(vdc_stats[\"Complete_bldg_total\"].values), color ='b', edgecolor ='grey', label ='Complete damage state')\n",
        "plt.bar(br1, np.sum(vdc_stats[\"Collapse_bldg_total\"].values), color ='r', edgecolor ='grey', label ='Collapse')\n",
        " \n",
        "plt.ylabel('Count', fontweight ='bold', fontsize = 15)\n",
        " \n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaJkW1yD4bYX"
      },
      "source": [
        "## Part III: Fatality modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxiUmAWY4db3"
      },
      "source": [
        "Now that you have an estimate of the number of buildings collapsing, you can start to model how many people might be killed in each VDC. For this exercise, I want you to assume a night-time scenario, where everybody is indoors at the time of the earthquake.\n",
        "\n",
        "First, you need to estimate how many people in each VDC live in collapsed buildings. Without any other knowledge, you should assume that the population in each VDC is equally distributed between all types of buildings. You therefore need to divide the total population in each VDC by the total number of buildings to calculate the average number of people per building.\n",
        "\n",
        "Let's create a new column *Pop_per_house* in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-0DQd0IxI0F"
      },
      "outputs": [],
      "source": [
        "vdc_stats[\"Pop_per_house\"] = vdc_stats[\"Pop_Total\"] / vdc_stats[\"total building count\"]\n",
        "\n",
        "display(vdc_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41JwFi2yxI0F"
      },
      "source": [
        "\n",
        "Now you can model the number of fatalities using Fatality Rates. The number of people killed by collapsing buildings varies by building type – buildings comprised of heavier material kill more people than buildings comprised of light material. Using the Fatality Rates provided below, calculate the total number of people killed in each building type in columns, then sum up to find the total number of fatalities in each VDC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdAkrzLhxI0F"
      },
      "source": [
        "| Building Type      | Fatality Rate (%) |\n",
        "| ----------- | ----------- |\n",
        "| WDN      | 0.5%       |\n",
        "| RCNE   | 10%        |\n",
        "| BCF      | 5%       |\n",
        "| BCR   | 15%        |\n",
        "| SM   | 5%        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBntAEi-xI0F"
      },
      "outputs": [],
      "source": [
        "# Collapse building * Falality Rate\n",
        "\n",
        "Fatality_bldg_WDN = Collapse_bldg_WDN * 0.005\n",
        "Fatality_bldg_RCNE = Collapse_bldg_RCNE * 0.10\n",
        "Fatality_bldg_BCF = Collapse_bldg_BCF * 0.05\n",
        "Fatality_bldg_BCR = Collapse_bldg_BCR * 0.15\n",
        "Fatality_bldg_SM = Collapse_bldg_SM * 0.05\n",
        "\n",
        "vdc_stats[\"Fatalities\"] = Fatality_bldg_WDN + Fatality_bldg_RCNE + Fatality_bldg_BCF + Fatality_bldg_BCR + Fatality_bldg_SM\n",
        "vdc_stats[\"Fatalities\"] = [round(x) for x in vdc_stats[\"Fatalities\"]]\n",
        "\n",
        "\n",
        "display(vdc_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s0ET12h4pgy"
      },
      "source": [
        "Plot the total number of fatalities per building type. We use the plotly library to get interactive plots (feel free to try others libraries if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQKmThCmxI0G"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "Total_Fatalities = np.sum(vdc_stats.Fatalities)\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "T = ['WDN', 'RCNE', 'BCF', 'BCR', 'SM']\n",
        "F = [np.sum(Fatality_bldg_WDN), np.sum(Fatality_bldg_RCNE), np.sum(Fatality_bldg_BCF),\n",
        "     np.sum(Fatality_bldg_BCR), np.sum(Fatality_bldg_SM)]\n",
        "\n",
        "\n",
        "# Add traces\n",
        "fig = go.Figure(data=[go.Bar(x=T, y=F)])\n",
        "fig.update_layout(title_text='Fatalities by Building typologies')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIKY-a-P4sAJ"
      },
      "source": [
        "## Part IV: Distributions and comparisons with past events"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 2015 Gorkha earthquake occurred on 25 April 2015 at 11:56 am NST (06:11:26 UTC) at a depth of approximately 8.2 km (5.1 mi) (which is considered shallow and therefore more damaging than quakes that originate deeper in the ground), with its epicentre approximately 34 km (21 mi) east-southeast of Lamjung, Nepal, lasting approximately fifty seconds. The earthquake was initially reported as 7.5 Mw by the United States Geological Survey (USGS) before it was quickly upgraded to 7.8 Mw"
      ],
      "metadata": {
        "id": "kNfbqZsh9Mxa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTNZbH4R40KR"
      },
      "source": [
        "Using the step and data that you are now familiar with and the data from *2015_EQData.csv* showing the fatalities that would have resulted from the 2015 Gorkha earthquake if it had happened at night-time. The results suggest a total of ~36,000 people would have been killed, ~27,000 more than were actually killed by the daytime event.\n",
        "\n",
        "Here are some functions that could prove useful for your assignment.\n",
        "\n",
        "```\n",
        "# Load 2015 data\n",
        "eq2015 = pd.read_csv(\"2015_EQData.csv\")\n",
        "\n",
        "# merging two dataset by using:\n",
        "merge = vdc_stats.merge(eq2015, left_on='VDC_NAME', right_on='VDC')\n",
        "\n",
        "```\n",
        "\n",
        "Please refer to the beginning of this notebook for the assignment tasks. Feel free to use your notebook or QGIS to create maps and plots (you can use screenshots to easily copy to your external document).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "mh",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "9bc1784a509eb05d4b5b2c557501ba384e2d87548a9872a84f56c34c0b30b0c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}